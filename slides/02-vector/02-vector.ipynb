{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Vectors (BV Chapters 1, 3, 5)'\n",
    "subtitle: Biostat 216\n",
    "author: Dr. Hua Zhou @ UCLA\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    embed-resources: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System information (for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.9.3\n",
      "Commit bed2cd540a1 (2023-08-24 14:43 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (arm64-apple-darwin22.4.0)\n",
      "  CPU: 12 × Apple M2 Max\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-14.0.6 (ORCJIT, apple-m1)\n",
      "  Threads: 2 on 8 virtual cores\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/github.com/ucla-biostat-216/2023fall/slides/02-vector`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Documents/github.com/ucla-biostat-216/2023fall/slides/02-vector/Project.toml`\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.3.2\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.24.2\n",
      "  \u001b[90m[b964fa9f] \u001b[39mLaTeXStrings v1.3.0\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.39.0\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.9.0\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, LaTeXStrings, LinearAlgebra, Plots, Random, Statistics\n",
    "Random.seed!(216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors (BV Chapters 1, 3, 5)\n",
    "\n",
    "## Notation\n",
    "\n",
    "- A (column) vector $\\mathbf{x} \\in \\mathbb{R}^n$:\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{pmatrix} = (x_1, x_2, \\ldots, x_n).\n",
    "$$  \n",
    "  - $n$ is the **dimension** (or **length** or **size**) of the vector. \n",
    "\n",
    "  - $x_i$ is the $i$-th **entry** (or **element** or **component** or **coefficient**) of $\\mathbf{x}$. \n",
    "\n",
    "  - A vector of length $n$ is called an **$n$-vector**.\n",
    "\n",
    "- A row vector $\\mathbf{x}' = (x_1 \\, x_2 \\, \\ldots x_n)$ (without commas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       "  1\n",
       " -1\n",
       "  2\n",
       "  0\n",
       "  3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a (column) vector in Julia\n",
    "x = [1, -1, 2, 0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×5 adjoint(::Vector{Int64}) with eltype Int64:\n",
       " 1  -1  2  0  3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row vector\n",
    "x'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subvector: $\\mathbf{x}_{2:4} = \\begin{pmatrix} x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " -1\n",
       "  2\n",
       "  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacked vector:\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "\\mathbf{y} \\\\ \\mathbf{z}\n",
    "\\end{pmatrix} = (\\mathbf{y}, \\mathbf{z}) = \\begin{pmatrix}\n",
    "y_1 \\\\ \\vdots \\\\ y_m \\\\ z_1 \\\\ \\vdots \\\\ z_n\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2, 3]\n",
    "z = [4, 5]\n",
    "x = [y; z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some special vectors\n",
    "\n",
    "- Zero vector: \n",
    "$$\n",
    "\\mathbf{0}_n = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}.\n",
    "$$\n",
    "The subscript denotes the length of the vector; it sometimes is omitted if obvious from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One vector: \n",
    "$$\n",
    "\\mathbf{1}_n = \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The unit vector (or elementary basis vector), $\\mathbf{e}_i$, has all zero entries except the $i$-th entry being 1. Note the length of $\\mathbf{e}_i$ is often implied by context.\n",
    "$$\n",
    "\\mathbf{e}_1 = \\begin{pmatrix}\n",
    "1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0\n",
    "\\end{pmatrix}, \\quad \\mathbf{e}_2 = \\begin{pmatrix}\n",
    "0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0\n",
    "\\end{pmatrix}, \\quad \\ldots, \\quad \\mathbf{e}_n = \\begin{pmatrix}\n",
    "0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a unit vector by comprehension\n",
    "e3 = [i == 3 ? 1 : 0 for i in 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector operations\n",
    "\n",
    "- **Vector addition** (or **elementwise addition**) and **vector substraction** (or **elementwise substraction**). For two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ (of same length),\n",
    "$$\n",
    "\\mathbf{x} + \\mathbf{y} = \\begin{pmatrix}\n",
    "x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n\n",
    "\\end{pmatrix}, \\quad \\mathbf{x} - \\mathbf{y} = \\begin{pmatrix}\n",
    "x_1 - y_1 \\\\ \\vdots \\\\ x_n - y_n\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       "  7\n",
       "  9\n",
       " 11\n",
       " 13\n",
       " 15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [6, 7, 8, 9, 10]\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       " -5\n",
       " -5\n",
       " -5\n",
       " -5\n",
       " -5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scalar-vector multiplication**. For a scalar $\\alpha \\in \\mathbb{R}$ and a vector $\\mathbf{x} \\in \\mathbb{R}^n$,\n",
    "$$\n",
    "\\alpha \\mathbf{x} = \\begin{pmatrix}\n",
    "\\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.5\n",
       " 1.0\n",
       " 1.5\n",
       " 2.0\n",
       " 2.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α = 0.5\n",
    "x = [1, 2, 3, 4, 5]\n",
    "α * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elementwise multiplication** or **Hadamard product**. For two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ (of same length),\n",
    "$$\n",
    "\\mathbf{x} \\circ \\mathbf{y} = \\begin{pmatrix}\n",
    "x_1 y_1 \\\\ \\vdots \\\\ x_n y_n\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       "  6\n",
       " 14\n",
       " 24\n",
       " 36\n",
       " 50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Julia, dot operation is elementwise operation\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [6, 7, 8, 9, 10]\n",
    "x .* y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For scalars $\\alpha_1, \\ldots, \\alpha_k \\in \\mathbb{R}$ and vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_k \\in \\mathbb{R}^n$, the **linear combination**\n",
    "$$\n",
    "\\sum_{i=1}^k \\alpha_i \\mathbf{x}_i = \\alpha_1 \\mathbf{x}_1 + \\cdots + \\alpha_k \\mathbf{x}_k\n",
    "$$\n",
    "is a sum of scalar-vector products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       "  4.0\n",
       "  5.5\n",
       "  7.0\n",
       "  8.5\n",
       " 10.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [6, 7, 8, 9, 10]\n",
    "1 * x + 0.5 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Vector transpose**:\n",
    "$$\n",
    "\\mathbf{x}' = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}' = (x_1 \\, \\ldots \\, x_n).\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "    \n",
    "1. $(\\mathbf{x}')' = \\mathbf{x}$.  \n",
    "    \n",
    "2. Transposition is a linear operator: $(\\alpha \\mathbf{x} + \\beta \\mathbf{y})' = \\alpha \\mathbf{x}' + \\beta \\mathbf{y}'$.  \n",
    "\n",
    "Superscript $^t$ or $^T$ is also commonly used to denote transpose: $\\mathbf{x}^t$, $\\mathbf{x}^T$.\n",
    "\n",
    "- The **inner product** or **dot product** between two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ is\n",
    "$$\n",
    "\\mathbf{x}' \\mathbf{y} = (x_1 \\, \\ldots \\, x_n) \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} = x_1 y_1 + \\cdots + x_n y_n.\n",
    "$$\n",
    "Other notations for inner products: $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, $\\sum_{i=1}^n x_i y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5]\n",
    "y = [6, 7, 8, 9, 10]\n",
    "x'y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot function is avaiable from the standard library Linear Algebra\n",
    "dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of inner product: \n",
    "\n",
    "1. $\\mathbf{x}' \\mathbf{x} = x_1^2 + \\cdots + x_n^2 \\ge 0$.\n",
    "\n",
    "2. $\\mathbf{x}' \\mathbf{x} = 0$ if and only if $\\mathbf{x} = \\mathbf{0}$.\n",
    "\n",
    "3. Commutative: $\\mathbf{x}' \\mathbf{y} = \\mathbf{y}' \\mathbf{x}$.\n",
    "\n",
    "4. Associative with scalar multiplication: $(\\alpha \\mathbf{x})' \\mathbf{y} = \\alpha (\\mathbf{x}' \\mathbf{y}) = \\mathbf{x}' (\\alpha \\mathbf{y})$. \n",
    "\n",
    "5. Distributive with vector addition: $(\\mathbf{x} + \\mathbf{y})' \\mathbf{z} = \\mathbf{x}' \\mathbf{z} + \\mathbf{y}' \\mathbf{z}$.\n",
    "\n",
    "Examples of inner product. **In class exercises**: express the following quantities using vector inner products of a vector $\\mathbf{x} \\in \\mathbb{R}^n$ and another vector.\n",
    "\n",
    "1. Individual component: $x_i = \\rule[-0.1cm]{1cm}{0.15mm}$.\n",
    "\n",
    "2. Differencing: $x_i - x_j = \\rule[-0.1cm]{1cm}{0.15mm}$.\n",
    "\n",
    "3. Sum: $x_1 + \\cdots + x_n = \\rule[-0.1cm]{1cm}{0.15mm}$.\n",
    "\n",
    "4. Average: $(x_1 + \\cdots + x_n) / n = \\rule[-0.1cm]{1cm}{0.15mm}$.\n",
    "\n",
    "## Computer representation of numbers and vectors\n",
    "\n",
    "- Real numbers are represented as the **floating point numbers** in computers.\n",
    "\n",
    "- (Double precision) floating point numbers approximate real numbers to an accuracy of 15-16 digits.\n",
    "\n",
    "- One (double precision) floating point number takes 64 bits (0s and 1s), or 8 bytes.\n",
    "\n",
    "- An $n$-vector requires $8n$ bytes to store.\n",
    "\n",
    "## Computational complexity of vector operations\n",
    "\n",
    "- One **floating point operation** (flop) is one basic arithmetic operation in $\\mathbb{R}$ or $\\mathbb{C}$: $+, -, *, /, \\sqrt, ...$\n",
    "\n",
    "- The **flop count** or **operation count** is the total number of flops in an algorithm.\n",
    "\n",
    "- A (very) crude predictor of run time of the algorithm is \n",
    "$$\n",
    "\\text{run time} \\approx \\frac{\\text{flop count}}{\\text{computer speed (flops/second)}}.\n",
    "$$\n",
    "\n",
    "- **Dominant term**: the highest-order term in the flop count.\n",
    "$$\n",
    "\\frac 13 n^3 + 100 n^2 + 10n + 5 \\approx \\frac 13 n^3.\n",
    "$$\n",
    "\n",
    "- **Order**: the power in the dominant term.\n",
    "$$\n",
    "\\frac 13 n^3 + 100 n^2 + 10n + 5 = \\text{order } n^3 = O(n^3).\n",
    "$$\n",
    "\n",
    "\n",
    "- **In-class exercises**. Assume vectors are all of length $n$. Give the flop counts of the following operations.\n",
    "\n",
    "    1. Sum the elements of a vector: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.\n",
    "    \n",
    "    2. Vector addition and substraction: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.\n",
    "\n",
    "    3. Scalar multiplication: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.\n",
    "\n",
    "    4. Elementwise multiplication: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.\n",
    "\n",
    "    5. Inner product: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.  \n",
    "    \n",
    "    6. Norm of a vector: $\\rule[-0.1cm]{1cm}{0.15mm}$ flops.\n",
    "    \n",
    "- These vector operations are all order $n$ algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.9.3\n",
      "Commit bed2cd540a1 (2023-08-24 14:43 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (arm64-apple-darwin22.4.0)\n",
      "  uname: Darwin 23.0.0 Darwin Kernel Version 23.0.0: Fri Sep 15 14:43:05 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6020 arm64 arm\n",
      "  CPU: Apple M2 Max: \n",
      "                 speed         user         nice          sys         idle          irq\n",
      "       #1-12  2400 MHz    1465885 s          0 s     773574 s   26205121 s          0 s\n",
      "  Memory: 96.0 GB (50415.953125 MB free)\n",
      "  Uptime: 369108.0 sec\n",
      "  Load Avg:  2.779296875  2.48828125  2.7109375\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-14.0.6 (ORCJIT, apple-m1)\n",
      "  Threads: 2 on 8 virtual cores\n",
      "Environment:\n",
      "  XPC_FLAGS = 0x0\n",
      "  PATH = /Applications/Julia-1.9.app/Contents/Resources/julia/bin:/Users/huazhou/.julia/conda/3/aarch64/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Applications/quarto/bin\n",
      "  TERM = xterm-256color\n",
      "  HOME = /Users/huazhou\n",
      "  FONTCONFIG_PATH = /Users/huazhou/.julia/artifacts/e6b9fb44029423f5cd69e0cbbff25abcc4b32a8f/etc/fonts\n"
     ]
    }
   ],
   "source": [
    "# info about my computer \n",
    "versioninfo(verbose = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that one Apple M2 performance core can do 8 double-precision flops per CPU cylce (?) at 2.4GHz (cycles/second). Then the theoretical throughput of a single performance core on my laptop is\n",
    "$$\n",
    "8 \\times 10^9 \\times 2.4 = 19.2 \\times 10^9 \\text{ flops/second} = 19.2 \\text{ GFLOPS}\n",
    "$$\n",
    "in double precision. I estimate my computer takes about\n",
    "$$\n",
    "\\frac{10^7 - 1}{19.2 \\times 10^9} \\approx 0.00052 \\text{ seconds} = 520 \\text{ micro seconds}\n",
    "$$\n",
    "to sum a vector of length $n = 10^7$ using a single performance core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001599 seconds (1 allocation: 16 bytes)\n"
     ]
    }
   ],
   "source": [
    "# the actual run time\n",
    "n = 10^7\n",
    "x = randn(n)\n",
    "sum(x) # compile\n",
    "@time sum(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark sum($x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm, distance, angle\n",
    "\n",
    "- The **Euclidean norm** or **L2 norm** of a vector $\\mathbf{x} \\in \\mathbb{R}^n$ is\n",
    "$$\n",
    "\\|\\mathbf{x}\\| = \\|\\mathbf{x}\\|_2 = (\\mathbf{x}'\\mathbf{x})^{1/2} = \\sqrt{x_1^2 + \\cdots + x_n^2}.\n",
    "$$\n",
    "The Euclidean/L2 norm captures the Euclidean length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "\n",
    "plot([0; x[1]], [0; x[2]], arrow = true, color = :blue,\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [((x .+ 0.3)..., L\"x=(%$(x[1]), %$(x[2]))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **L1 norm** of a vector $\\mathbf{x} \\in \\mathbb{R}^n$ is\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_1 = |x_1| + \\cdots + |x_n|.\n",
    "$$\n",
    "Also known as **Manhattan Distance** or **Taxicab norm**. The L1 norm is the distance you have to travel between the origin $\\mathbf{0}_n$ to the destination $\\mathbf{x} = (x_1, \\ldots, x_n)'$, in a way that resembles how a taxicab drives between city blocks to arrive at its destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "\n",
    "plot([0 x[1]; x[1] x[1]], [0 0; 0 x[2]], arrow = true, color = :blue,\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [((x .+ 0.3)..., L\"x=(%$(x[1]), %$(x[2]))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Properties of L2 norm.\n",
    "\n",
    "    1. Positive definiteness: $\\|\\mathbf{x}\\| \\ge 0$ for any vector $\\mathbf{x}$. $\\|\\mathbf{x}\\| = 0$ if and only if $\\mathbf{x}=\\mathbf{0}$.\n",
    "    \n",
    "    2. Homogeneity: $\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|$ for any scalar $\\alpha$ and vector $\\mathbf{x}$.  \n",
    "    \n",
    "    3. Triangle inequality: $\\|\\mathbf{x} + \\mathbf{y}\\| \\le \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|$ for any $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$.  \n",
    "    Proof: use Cauchy-Schwartz inequality. TODO in class.\n",
    "    \n",
    "    4. **Cauchy-Schwarz inequality**: $|\\mathbf{x}' \\mathbf{y}| \\le \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|$ for any $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$. The equality holds when (1) $\\mathbf{x} = \\mathbf{0}$ or $\\mathbf{y}=\\mathbf{0}$ or (2) $\\mathbf{x} \\ne \\mathbf{0}$, $\\mathbf{y} \\ne \\mathbf{0}$, and $\\mathbf{x} = \\alpha \\mathbf{y}$ for some $\\alpha \\ne 0$.\n",
    "    \n",
    "    Proof: The function $f(t) = \\|\\mathbf{x} - t \\mathbf{y}\\|_2^2 = \\|\\mathbf{x}\\|_2^2 - 2t (\\mathbf{x}' \\mathbf{y}) + t^2\\|\\mathbf{y}\\|_2^2$ is minimized at $t^\\star =(\\mathbf{x}'\\mathbf{y}) / \\|\\mathbf{y}\\|^2$ with minimal value $0 \\le f(t^\\star) = \\|\\mathbf{x}\\|^2 - (\\mathbf{x}'\\mathbf{y})^2 / \\|\\mathbf{y}\\|^2$.\n",
    "\n",
    "    There are at least 5 other proofs of CS inequality on [Wikipedia](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality).\n",
    "\n",
    "The first there properties are the defining properties of any vector norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "y = [0.5, 1]\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = :blue,\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-1, 3),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] + 0.2, L\"y=(%$(y[1]), %$(y[2]))'\")],\n",
    "    xticks = -3:1:3, yticks = -1:1:3,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "\n",
    "plot!([x[1] 0; x[1] + y[1] x[1] + y[1]], [x[2] 0; x[2] + y[2] x[2] + y[2]], \n",
    "    arrow = true, linestyle = :dash, color = :blue, linealpha = 0.5,\n",
    "    annotations = [(x[1] + y[1] - 0.2, \n",
    "                    x[2] + y[2] + 0.2, \n",
    "                    L\"x+y=(%$(x[1] + y[1]), %$(x[2] + y[2]))'\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check triangular inequality on random vectors\n",
    "@show x = randn(5)\n",
    "@show y = randn(5)\n",
    "@show norm(x + y)\n",
    "@show norm(x) + norm(y)\n",
    "@show norm(x + y) ≤ norm(x) + norm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Cauchy-Schwarz inequality on random vectors\n",
    "@show x = randn(5)\n",
    "@show y = randn(5)\n",
    "@show abs(x'y)\n",
    "@show norm(x) * norm(y)\n",
    "@show abs(x'y) ≤ norm(x) * norm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **(Euclidean) distance** between vectors $\\mathbf{x}$ and $\\mathbf{y}$ is defined as $\\|\\mathbf{x} - \\mathbf{y}\\|$. \n",
    "\n",
    "- Property of distances. \n",
    "\n",
    "    1. Nonnegativity. $\\|\\mathbf{x} - \\mathbf{y}\\| \\ge 0$ for all $\\mathbf{x}$ and $\\mathbf{y}$. And $\\|\\mathbf{x} - \\mathbf{y}\\| = 0$ if and only if $\\mathbf{x} = \\mathbf{y}$.\n",
    "    \n",
    "    2. Triangular inequality: $\\|\\mathbf{x} - \\mathbf{y}\\| \\le \\|\\mathbf{x} - \\mathbf{z}\\| + \\|\\mathbf{z} - \\mathbf{y}\\|$.  \n",
    "    Proof: TODO.\n",
    "\n",
    "- The **average** of a vector $\\mathbf{x}$ is\n",
    "$$\n",
    "\\operatorname{avg}(\\mathbf{x}) = \\bar{\\mathbf{x}} = \\frac{x_1 + \\cdots + x_n}{n} = \\frac{\\mathbf{1}' \\mathbf{x}}{n}.\n",
    "$$\n",
    "\n",
    "- The **rooted mean square** (RMS) of a vector is\n",
    "$$\n",
    "\\operatorname{rms}(\\mathbf{x}) = \\sqrt{\\frac{x_1^2 + \\cdots + x_n^2}{n}} = \\frac{\\|\\mathbf{x}\\|}{\\sqrt n}.\n",
    "$$\n",
    "\n",
    "- The **standard deviation** of a vector $\\mathbf{x}$ is\n",
    "$$\n",
    "\\operatorname{std}(\\mathbf{x}) = \\sqrt{\\frac{(x_1 - \\bar{\\mathbf{x}})^2 + \\cdots + (x_n - \\bar{\\mathbf{x}})^2}{n}} = \\operatorname{rms}(\\mathbf{x} - \\bar{\\mathbf{x}} \\mathbf{1}) = \\frac{\\|\\mathbf{x} - (\\mathbf{1}' \\mathbf{x} / n) \\mathbf{1}\\|}{\\sqrt n}.\n",
    "$$\n",
    "\n",
    "- Theorem: $\\operatorname{avg}(\\mathbf{x})^2 + \\operatorname{std}(\\mathbf{x})^2 = \\operatorname{rms}(\\mathbf{x})^2$.\n",
    "\n",
    "    This result underlies the famous _bias-variance tradeoff_ in statistics. \n",
    "    \n",
    "    Proof: HW1. Hint: $\\operatorname{std}(\\mathbf{x})^2 = \\frac{\\|\\mathbf{x} - (\\mathbf{1}' \\mathbf{x} / n) \\mathbf{1}\\|^2}{n} = ... = \\operatorname{rms}(\\mathbf{x})^2 - \\operatorname{avg}(\\mathbf{x})^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(5)\n",
    "@show mean(x)^2 + std(x, corrected = false)^2\n",
    "@show norm(x)^2 / length(x)\n",
    "# floating point arithmetics is not exact\n",
    "@show mean(x)^2 + std(x, corrected = false)^2 ≈ norm(x)^2 / length(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Angle** between two nonzero vectors $\\mathbf{x}, \\mathbf{y}$ is\n",
    "$$\n",
    "\\theta = \\angle (\\mathbf{x}, \\mathbf{y}) = \\operatorname{arccos} \\left(\\frac{\\mathbf{x}'\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\\right).\n",
    "$$\n",
    "This is the unique value of $\\theta \\in [0, \\pi]$ that satisifies\n",
    "$$\n",
    "\\cos \\theta = \\frac{\\mathbf{x}'\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}.\n",
    "$$\n",
    "\n",
    "- Cauchy-Schwarz inequality guarantees that \n",
    "$$\n",
    "-1 \\le \\cos \\theta = \\frac{\\mathbf{x}'\\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|} \\le 1.\n",
    "$$\n",
    "\n",
    "- Terminology\n",
    "\n",
    "| Angle                    | Sign of inner product                                   | Terminology                                                                        |\n",
    "|--------------------------|---------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| $\\theta = 0$             | $\\mathbf{x}'\\mathbf{y}=\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|$    | $\\mathbf{x}$ and $\\mathbf{y}$ are aligned or parallel                             |\n",
    "| $0 \\le \\theta < \\pi/2$   | $\\mathbf{x}'\\mathbf{y} > 0$                             |  $\\mathbf{x}$ and $\\mathbf{y}$ make an acute angle                                |\n",
    "| $\\theta = \\pi / 2$       | $\\mathbf{x}'\\mathbf{y} = 0$                             | $\\mathbf{x}$ and $\\mathbf{y}$ are **orthogonal**, $\\mathbf{x} \\perp \\mathbf{y}$ |\n",
    "| $\\pi/2 < \\theta \\le \\pi$ | $\\mathbf{x}'\\mathbf{y} < 0$                             | $\\mathbf{x}$ and $\\mathbf{y}$ make an obtuse angle                                |\n",
    "| $\\theta = \\pi$           | $\\mathbf{x}'\\mathbf{y} = -\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|$ | $\\mathbf{x}$ and $\\mathbf{y}$ are anti-aligned or opposed                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "y = 0.5x\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = [:blue, :purple],\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] + 0.2, L\"y=(%$(y[1]), %$(y[2]))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    title = L\"$x$ and $y$ are aligned or parallel: \n",
    "    $x'y = %$(dot(x, y)) = \\|\\|x\\|\\| \\|\\|y\\|\\|$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "θ = π/4\n",
    "A = [cos(θ) -sin(θ); sin(θ) cos(θ)]\n",
    "y = 0.5(A * x)\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = [:blue, :purple],\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] + 0.2, L\"y=(%$(round(y[1], digits=1)), %$(round(y[2], digits=1)))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    title = L\"$x$ and $y$ make an acute angle: \n",
    "    $0 < x'y = %$(round(dot(x, y), digits=1)) < %$(round(norm(x) * norm(y), digits=1)) = \\|\\|x\\|\\| \\|\\|y\\|\\|$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "θ = π/2\n",
    "A = [cos(θ) -sin(θ); sin(θ) cos(θ)]\n",
    "y = 0.5(A * x)\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = [:blue, :purple],\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] + 0.2, L\"y=(%$(round(y[1], digits=1)), %$(round(y[2], digits=1)))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    title = L\"$x$ is orthogonal to $y$ ($x \\perp y$): \n",
    "    $x'y = %$(round(dot(x, y), digits=1))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "θ = (3/4)π\n",
    "A = [cos(θ) -sin(θ); sin(θ) cos(θ)]\n",
    "y = 0.5(A * x)\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = [:blue, :purple],\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] + 0.2, L\"y=(%$(round(y[1], digits=1)), %$(round(y[2], digits=1)))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    title = L\"$x$ and $y$ make an obtuse angle: \n",
    "    $- \\|\\|x\\|\\| \\|\\|y\\|\\| = -%$(round(norm(x) * norm(y), digits=1)) < %$(round(dot(x, y), digits=1)) = x'y < 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "x = [2, 1]\n",
    "θ = π\n",
    "A = [cos(θ) -sin(θ); sin(θ) cos(θ)]\n",
    "y = 0.5(A * x)\n",
    "\n",
    "plot([0 0; x[1] y[1]], [0 0; x[2] y[2]], arrow = true, color = [:blue, :purple],\n",
    "    legend = :none, xlims = (-3, 3), ylims = (-2, 2),\n",
    "    annotations = [(x[1] + 0.6, x[2], L\"x=(%$(x[1]), %$(x[2]))'\"),\n",
    "        (y[1] - 0.25, y[2] - 0.2, L\"y=(%$(round(y[1], digits=1)), %$(round(y[2], digits=1)))'\")],\n",
    "    xticks = -3:1:3, yticks = -2:1:2,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    title = L\"$x$ and $y$ are anti-aligned or opposed: \n",
    "    $x'y = %$(round(dot(x, y), digits=1)) = -\\|\\|x\\|\\| \\|\\|y\\|\\|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear independence\n",
    "\n",
    "- Example: Consider vectors in $\\mathbb{R}^3$\n",
    "$$\n",
    "\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}.\n",
    "$$\n",
    "The fourth vector $\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$ is in some sense redundant because it can be expressed as a linear combination of the other 3 vectors\n",
    "$$\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 2 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + 3 \\cdot \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n",
    "$$\n",
    "Similarly, each one of these 4 vectors can be expressed as the linear combination of the other 3. We say these four vectors are linearly dependent. \n",
    "\n",
    "- A set of vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k \\in \\mathbb{R}^n$ are **linearly dependent** if there exist constants $\\alpha_1, \\ldots, \\alpha_k$, which are not all zeros, such that \n",
    "$$\n",
    "\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k = \\mathbf{0}.\n",
    "$$\n",
    "They are **linearly independent** if they are not linearly dependent. That is if $\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k = \\mathbf{0}$ then $\\alpha_1 = \\cdots = \\alpha_k = 0$ (this is usually how we show that a set of vectors are linearly independent).\n",
    "\n",
    "- Theorem: Unit vectors $\\mathbf{e}_1, \\ldots, \\mathbf{e}_n \\in \\mathbb{R}^n$ are linearly independent. \n",
    "\n",
    "    Proof: TODO in class.\n",
    "    \n",
    "- Theorem: If $\\mathbf{x}$ is a linear combination of linearly independent vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k$. That is $\\mathbf{x} = \\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k$. Then the coefficients $\\alpha_1, \\ldots, \\alpha_k$ are unique. \n",
    "\n",
    "    Proof: TODO in class. Hint: proof by contradition.\n",
    "    \n",
    "- **Independence-dimension inequality** or **order-dimension inequality**. If the vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k \\in \\mathbb{R}^n$ are linearly independent, then $k \\le n$.\n",
    "\n",
    "    In words, there can be at most $n$ linearly independent vectors in $\\mathbb{R}^n$. Or any set of $n+1$ or more vectors in $\\mathbb{R}^n$ must linearly dependent.\n",
    "\n",
    "    Proof (optional): We show this by induction. Let $a_1, \\ldots, a_k \\in \\mathbb{R}^1$ be linearly independent. We must have $a_1 \\ne 0$. This means that every element $a_i$ of the collection can be expressed as a multiple of $a_i = (a_i / a_1) a_1$ of the first element $a_1$. This contradicts the linear independence thus $k$ must be 1.  \n",
    "    Induction hypothesis: suppose $n \\ge 2$ and the independence-dimension inequality holds for $k \\le n$. We partition the vectors $\\mathbf{a}_i \\in \\mathbb{R}^n$ as\n",
    "$$\n",
    "    \\mathbf{a}_i = \\begin{pmatrix} \\mathbf{b}_i \\\\ \\alpha_i \\end{pmatrix}, \\quad i = 1,\\ldots,k,\n",
    "$$\n",
    "where $\\mathbf{b}_i \\in \\mathbb{R}^{n-1}$ and $\\alpha_i \\in \\mathbb{R}$.  \n",
    "    First suppose $\\alpha_1 = \\cdots = \\alpha_k = 0$. Then the vectors $\\mathbf{b}_1, \\ldots, \\mathbf{b}_k$ are linearly independent: $\\sum_{i=1}^k \\beta_i \\mathbf{b}_i = \\mathbf{0}$ if and only if $\\sum_{i=1}^k \\beta_i \\mathbf{a}_i = \\mathbf{0}$, which is only possible for $\\beta_1 = \\cdots = \\beta_k = 0$ because the vectors $\\mathbf{a}_i$ are linearly independent. The vectors $\\mathbf{b}_i$ therefore form a linearly independent collection of $(n-1)$-vectors. By the induction hypothesis we have $k \\le n-1$ so $k \\le n$.  \n",
    "    Next we assume the scalars $\\alpha_i$ are not all zero. Assume $\\alpha_j \\ne 0$. We define a collection of $k-1$ vectors $\\mathbf{c}_i$ of length $n-1$ as follows:\n",
    "$$\n",
    "    \\mathbf{c}_i = \\mathbf{b}_i - \\frac{\\alpha_i}{\\alpha_j} \\mathbf{b}_j, \\quad i = 1, \\ldots, j-1, \\mathbf{c}_i = \\mathbf{b}_{i+1} - \\frac{\\alpha_{i+1}}{\\alpha_j} \\mathbf{b}_j, \\quad i = j, \\ldots, k-1.\n",
    "$$\n",
    "These $k-1$ vectors are linealy independent: If $\\sum_{i=1}^{k-1} \\beta_i c_i = 0$ then\n",
    "$$\n",
    "    \\sum_{i=1}^{j-1} \\beta_i \\begin{pmatrix} \\mathbf{b}_i \\\\ \\alpha_i \\end{pmatrix} + \\gamma \\begin{pmatrix} \\mathbf{b}_j \\\\ \\alpha_j \\end{pmatrix} + \\sum_{i=j+1}^k \\beta_{i-1} \\begin{pmatrix} \\mathbf{b}_i \\\\ \\alpha_i \\end{pmatrix} = \\mathbf{0}\n",
    "$$\n",
    "with $\\gamma = - \\alpha_j^{-1} \\left( \\sum_{i=1}^{j-1} \\beta_i \\alpha_i + \\sum_{i=j+1}^k \\beta_{i-1} \\alpha_i \\right)$. Since the vectors $\\mathbf{a}_i$ are linearly independent, all coefficients $\\beta_i$ and $\\gamma$ are all zero. This in turns implies that the vectors $\\mathbf{c}_1, \\ldots, \\mathbf{c}_{k-1}$ are linearly independent. By the induction hypothesis $k-1 \\le n-1$, we have established $k \\le n$.\n",
    "\n",
    "## Basis\n",
    "\n",
    "- A set of $n$ linearly independent vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathbb{R}^n$ is called a **basis** for $\\mathbb{R}^n$. \n",
    "\n",
    "- Fact: the zero vector $\\mathbf{0}_n$ cannot be a basis vector in $\\mathbb{R}^n$. Why?\n",
    "\n",
    "- Theorem: Any vector $\\mathbf{x} \\in \\mathbb{R}^n$ can be expressed as a linear combination of basis vectors $\\mathbf{x} = \\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_n \\mathbf{a}_n$ for some $\\alpha_1, \\ldots, \\alpha_n$, and these coefficients are unique. This is called expansion of $\\mathbf{x}$ in the basis $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n$.\n",
    "\n",
    "    Proof of existence by contradition (optional). Suppose $\\mathbf{x}$ can NOT be expressed as a linear combination of basis vectors. Suppose an arbitrary linear combination $\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_n \\mathbf{a}_n + \\beta \\mathbf{x} = \\mathbf{0}$. Then $\\beta = 0$ otherwise it contradictions with our assumption. Also $\\alpha_1 = \\cdots = \\alpha_n = 0$ by linear independence of $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n$. Therefore we conclude $\\alpha_1 = \\cdots = \\alpha_n = \\beta = 0$. Thus $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n, \\mathbf{x}$ are linearly independent, contradicting with the independence-dimension inequality.\n",
    "    \n",
    "    Proof of uniqueness: TODO in class.\n",
    "\n",
    "- Example: Unit vectors $\\mathbf{e}_1, \\ldots, \\mathbf{e}_n$ form a basis for $\\mathbb{R}^n$. Expansion of a vector $\\mathbf{x} \\in \\mathbb{R}^n$ in this basis is\n",
    "$$\n",
    "\\mathbf{x} = x_1 \\mathbf{e}_1 + \\cdots + x_n \\mathbf{e}_n.\n",
    "$$\n",
    "\n",
    "## Orthonormal basis\n",
    "\n",
    "- A set of vectors $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k$ are **(mutually) orthogonal** if $\\mathbf{a}_i \\perp \\mathbf{a}_j$ for any $i \\ne j$. They are **normalized** if $\\|\\mathbf{a}_i\\|=1$ for all $i$. They are **orthonormal** if they are both orthogonal and normalized.\n",
    "\n",
    "    Orthonormality is often expressed compactly by $\\mathbf{a}_i'\\mathbf{a}_j = \\delta_{ij}$, where \n",
    "$$\n",
    "\\delta_{ij} = \\begin{cases} \n",
    "1 & \\text{if } i = j \\\\ \n",
    "0 & \\text{if } i \\ne j \n",
    "\\end{cases}\n",
    "$$\n",
    "is the Kronecker delta notation.\n",
    "\n",
    "- Theorem: An orthonormal set of vectors are linearly independent.\n",
    "\n",
    "    Proof: TODO in class. Expand $\\|\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k\\|^2 = (\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k)'(\\alpha_1 \\mathbf{a}_1 + \\cdots + \\alpha_k \\mathbf{a}_k)$.\n",
    "    \n",
    "- By the independence-dimension inequality, must have $k \\le n$. When $k=n$, $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n$ are called an **orthonormal basis**. \n",
    "\n",
    "- Examples of orthonormal basis:\n",
    "\n",
    "    1. Unit vectors $\\mathbf{e}_1, \\ldots, \\mathbf{e}_n$ in $\\mathbb{R}^n$.  \n",
    "    2. The 3 vectors in $\\mathbb{R}^3$:\n",
    "$$\n",
    "\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\frac{1}{\\sqrt 2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad \\quad \\frac{1}{\\sqrt 2} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [0, 0, 1]\n",
    "a2 = (1 / sqrt(2)) * [1, 1, 0]\n",
    "a3 = (1 / sqrt(2)) * [1, -1, 0]\n",
    "@show norm(a1), norm(a2), norm(a3)\n",
    "@show a1'a2, a1'a3, a2'a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "plot3d([0; a1[1]], [0; a1[2]], [0; a1[3]], \n",
    "    arrow = true, legend = :none,\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal,\n",
    "    xlims = (-1, 1), ylims = (-1, 1), zlims = (-1, 1),\n",
    "    xticks = -1:1:1, yticks = -1:1:1, zticks = -1:1:1\n",
    ")\n",
    "plot3d!([0; a2[1]], [0; a2[2]], [0; a2[3]])\n",
    "plot3d!([0; a3[1]], [0; a3[2]], [0; a3[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Orthonormal expansion**. If $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n \\in \\mathbb{R}^n$ is an orthonormal basis, then for any vector $\\mathbf{x} \\in \\mathbb{R}^n$, \n",
    "$$\n",
    "\\mathbf{x} = (\\mathbf{a}_1'\\mathbf{x}) \\mathbf{a}_1 + \\cdots + (\\mathbf{a}_n'\\mathbf{x}) \\mathbf{a}_n.\n",
    "$$\n",
    "\n",
    "    Proof: Take inner product with $\\mathbf{a}_i$ on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show x = randn(3)\n",
    "@show (a1'x) * a1 + (a2'x) * a2 + (a3'x) * a3\n",
    "@show x ≈ (a1'x) * a1 + (a2'x) * a2 + (a3'x) * a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to orthonormalize a set of vectors? Gram-Schmidt algorithm\n",
    "\n",
    "- Given vectros $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k \\in \\mathbb{R}^n$. G-S algorithm generates a sequence of orthonormal vectors $\\mathbf{q}_1, \\mathbf{q}_2, \\ldots$.\n",
    "\n",
    "- For $i=1,\\ldots,k$:  \n",
    "\n",
    "    1. Orthogonalization: $\\tilde{\\mathbf{q}}_i = \\mathbf{a}_i - [(\\mathbf{q}_1' \\mathbf{a}_i) \\mathbf{q}_1 + \\cdots + (\\mathbf{q}_{i-1}' \\mathbf{a}_i) \\mathbf{q}_{i-1}]$.  \n",
    "    \n",
    "    2. Test for linear independence: if $\\tilde{\\mathbf{q}}_i = \\mathbf{0}$, quit.  \n",
    "    \n",
    "    3. Normalization: $\\mathbf{q}_i = \\tilde{\\mathbf{q}}_i / \\|\\tilde{\\mathbf{q}}_i\\|$.\n",
    "    \n",
    "- If G-S does not stop early (in step 2), $\\mathbf{a}_1, \\ldots, \\mathbf{a}_k$ are linearly independent. \n",
    "\n",
    "- If G-S stops early in iteration $i=j$, then $\\mathbf{a}_j$ is a linear combination of $\\mathbf{a}_1, \\ldots, \\mathbf{a}_{j-1}$ and $\\mathbf{a}_1, \\ldots, \\mathbf{a}_{j-1}$ are linearly independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "a1 = [0.5, 2]\n",
    "a2 = [1.5, 0.5]\n",
    "\n",
    "p1 = plot([0 0; a1[1] a2[1]], [0 0; a1[2] a2[2]], arrow = true, color = :purple,\n",
    "    axis = ([], false), legend = :none,\n",
    "    xlims = (-2, 2), ylims = (-2, 2),\n",
    "    annotations = [(a1[1] + 0.2, a1[2], L\"a_1\"),\n",
    "        (a2[1] + 0.2, a2[2], L\"a_2\")],\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "t = 0:0.01:2π\n",
    "plot!(p1, sin.(t), cos.(t), color = :gray)\n",
    "\n",
    "# orthogonalizing a1\n",
    "q̃1 = copy(a1) \n",
    "p2 = plot([0 0; q̃1[1] a2[1]], [0 0; q̃1[2] a2[2]], arrow = true, color = [:red :purple],\n",
    "    axis = ([], false), legend = :none,\n",
    "    xlims = (-2, 2), ylims = (-2, 2),\n",
    "    annotations = [(q̃1[1] + 0.2, q̃1[2], L\"\\tilde{q}_1\"),\n",
    "        (a2[1] + 0.2, a2[2], L\"a_2\")],\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "t = 0:0.01:2π\n",
    "plot!(p2, sin.(t), cos.(t), color = :gray)\n",
    "\n",
    "# normalizing q̃1\n",
    "q1 = q̃1 / norm(q̃1) \n",
    "p3 = plot([0 0; q1[1] a2[1]], [0 0; q1[2] a2[2]], arrow = true, color = [:green :purple],\n",
    "    axis = ([], false), legend = :none,\n",
    "    xlims = (-2, 2), ylims = (-2, 2),\n",
    "    annotations = [(q1[1] + 0.2, q1[2], L\"q_1\"),\n",
    "        (a2[1] + 0.2, a2[2], L\"a_2\")],\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "t = 0:0.01:2π\n",
    "plot!(p3, sin.(t), cos.(t), color = :gray)\n",
    "\n",
    "# orthogonalizing a2\n",
    "q̃2 = a2 - dot(a2, q1) * q1\n",
    "p4 = plot([0 0 0; q1[1] a2[1] q̃2[1]], [0 0 0; q1[2] a2[2] q̃2[2]], \n",
    "    arrow = true, color = [:green :purple :red],\n",
    "    xlims = (-1.5, 1.5), ylims = (-1.5, 1.5),\n",
    "    axis = ([], false), legend = :none,\n",
    "    annotations = [\n",
    "        (q1[1] + 0.2, q1[2], L\"q_1\"),\n",
    "        (a2[1] + 0.2, a2[2], L\"a_2\"),\n",
    "        (q̃2[1] + 0.2, q̃2[2] - 0.2, L\"\\tilde{q}_2\"),\n",
    "        ((a2[1] + q̃2[1]) / 2 + 1.2, (a2[2] + q̃2[2]) / 2, L\"-(q_1' a_2)q_1\")\n",
    "        ],\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "t = 0:0.01:2π\n",
    "plot!(p4, sin.(t), cos.(t), color = :gray)\n",
    "plot!(p4, [a2[1]; q̃2[1]], [a2[2]; q̃2[2]], arrow = true, color = :black)\n",
    "\n",
    "# normalizing q̃2\n",
    "q2 = q̃2 / norm(q̃2) \n",
    "p5 = plot([0 0; q1[1] q2[1]], [0 0; q1[2] q2[2]], arrow = true, color = :green,\n",
    "    axis = ([], false), legend = :none,\n",
    "    xlims = (-1.5, 1.5), ylims = (-1.5, 1.5),\n",
    "    annotations = [(q1[1] + 0.3, q1[2] + 0.1, L\"q_1\"),\n",
    "        (q2[1] + 0.3, q2[2], L\"q_2\")],\n",
    "    framestyle = :origin,\n",
    "    aspect_ratio = :equal)\n",
    "t = 0:0.01:2π\n",
    "plot!(p5, sin.(t), cos.(t), color = :gray)\n",
    "\n",
    "plot(p1, p2, p3, p4, p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 5, k = 3\n",
    "@show a1 = randn(5)\n",
    "@show a2 = randn(5)\n",
    "@show a3 = randn(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i = 1\n",
    "# orthogonalization\n",
    "@show q̃1 = copy(a1)\n",
    "# test for linear independence\n",
    "@show norm(q̃1) ≈ 0\n",
    "# normalization\n",
    "@show q1 = q̃1 / norm(q̃1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i = 2\n",
    "# orthogonalization\n",
    "@show q̃2 = a2 - (q1'a2) * q1\n",
    "# test for linear independence\n",
    "@show norm(q̃2) ≈ 0\n",
    "# normalization\n",
    "@show q2 = q̃2 / norm(q̃2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i = 3\n",
    "# orthogonalization\n",
    "@show q̃3 = a3 - (q1'a3) * q1 - (q2'a3) * q2\n",
    "# test for linear independence\n",
    "@show norm(q̃3) ≈ 0\n",
    "# Normalization\n",
    "@show q3 = q̃3 / norm(q̃3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for orthonormality of q1, q2, q3\n",
    "@show norm(q1), norm(q2), norm(q3)\n",
    "@show q1'q2, q1'q3, q2'q3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show by induction that $\\mathbf{q}_1, \\ldots, \\mathbf{q}_i$ are orthonormal (optional):\n",
    "\n",
    "- Assume it's true for $i-1$. \n",
    "\n",
    "- Orthogonalization step ensures that $\\tilde{\\mathbf{q}}_i \\perp \\mathbf{q}_1, \\ldots, \\tilde{\\mathbf{q}}_i \\perp \\mathbf{q}_{i-1}$. To show this, take inner product of both sides with $\\mathbf{q}_j$, $j < i$\n",
    "$$\n",
    "\\mathbf{q}_j' \\tilde{\\mathbf{q}}_i = \\mathbf{q}_j' \\mathbf{a}_i - (\\mathbf{q}_1' \\mathbf{a}_i) (\\mathbf{q}_j' \\mathbf{q}_1) - \\cdots - (\\mathbf{q}_{i-1}' \\mathbf{a}_i) (\\mathbf{q}_j' \\mathbf{q}_{i-1}) = \\mathbf{q}_j' \\mathbf{a}_i - \\mathbf{q}_j' \\mathbf{a}_i = 0.\n",
    "$$\n",
    "\n",
    "- So $\\mathbf{q}_1, \\ldots, \\mathbf{q}_i$ are orthogonal. The normalization step ensures $\\mathbf{q}_i$ is normal.\n",
    "\n",
    "Suppose G-S has not terminated by iteration $i$, then \n",
    "\n",
    "- $\\mathbf{a}_i$ is a combination of $\\mathbf{q}_1, \\ldots, \\mathbf{q}_i$, and\n",
    "\n",
    "- $\\mathbf{q}_i$ is a combination of $\\mathbf{a}_1, \\ldots, \\mathbf{a}_i$.\n",
    "\n",
    "Computational complexity of G-S algorithm:\n",
    "\n",
    "- Step 1 of iteration $i$ requires (1) $i-1$ inner products, $\\mathbf{q}_1' \\mathbf{a}_i, \\ldots, \\mathbf{q}_{i-1}' \\mathbf{a}_i$, which costs $(i-1)(2n-1)$ flops, (2) $2n(i-1)$ flops to compute $\\tilde{\\mathbf{q}}_i$\n",
    "\n",
    "- Step 2 of iteration $i$ requires less than $n$ flops.  \n",
    "\n",
    "- Step 3 of iteration $i$ requires $3n$ flops to normalize $\\|\\tilde{\\mathbf{q}}_i\\|$. \n",
    "\n",
    "- Assuming no early termination, total computational cost of the GS algorithm to orthonormalize a set of $k$ vectors in $\\mathbb{R}^n$ is:\n",
    "$$\n",
    "\\sum_{i=1}^k [(4n-1) (i - 1) + 3n] = (4n - 1) \\frac{k(k-1)}{2} + 3nk \\approx 2nk^2 = O(nk^2),\n",
    "$$\n",
    "using $\\sum_{i=1}^k (i - 1) = k(k-1)/2$. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66.52174377441406px",
    "width": "251.7391357421875px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
